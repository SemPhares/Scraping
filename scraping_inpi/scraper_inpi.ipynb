{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    " \n",
    "# service = Service('chromedriver')\n",
    "# service.start()\n",
    "# options = Options() #webdriver.chromeOptions()\n",
    "\n",
    "# prefs={\"download.default_directory\":\"./EnterpriseFiles\",\n",
    "#       \"download.prompt_for_download\": False,\n",
    "#       \"download.directory_upgrade\": True,\n",
    "#       \"safebrowsing.enabled\": True  }\n",
    "\n",
    "# options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "# driver = webdriver.Firefox(executable_path='geckodriver.exe')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create webdriver object\n",
    "driver = webdriver.Chrome(executable_path=\"../chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok 4\n",
      "downloading\n"
     ]
    }
   ],
   "source": [
    "# load the page\n",
    "driver.get(\"https://data.inpi.fr\")\n",
    "\n",
    "    #make the request\n",
    "\n",
    "req = driver.find_element_by_class_name(\"search-input\")\n",
    "req.clear()\n",
    "req.send_keys(\"Laurent Mignon\")\n",
    "req.submit()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#     driver.maximize_window()\n",
    "\n",
    "check_all_result = driver.find_element_by_xpath(\"//input[@type='checkbox'][@id='result-all']\")#[contains(@class='checkbox-result-bloc')]\n",
    "driver.execute_script(\"arguments[0].click();\", check_all_result)\n",
    "print(\"ok 1\")\n",
    "\n",
    "download_button = driver.find_element_by_xpath(\"//button[@title='Télécharger'][@class='btn btn-link p-0 actions-buttons']\")\n",
    "driver.execute_script(\"arguments[0].click();\", download_button)\n",
    "print(\"ok 2\")\n",
    "\n",
    "csv_selector = driver.find_element_by_id(\"export_companies_format_1\")\n",
    "driver.execute_script(\"arguments[0].click();\", csv_selector)\n",
    "print(\"ok 3\")\n",
    "\n",
    "# select_all_infos = driver.find_element_by_xpath(\"//input[@type='checkbox'][@name='select-all'][@class='select-all-input']\")\n",
    "select_all_infos = driver.find_element_by_xpath(\"//label[@for='select-all'][@class='select-all-label ml-1']\")\n",
    "driver.execute_script(\"arguments[0].click();\", select_all_infos)\n",
    "print(\"ok 4\")\n",
    "\n",
    "download = driver.find_element_by_id('export_companies_submit')\n",
    "time.sleep(1)\n",
    "download.click()\n",
    "# driver.execute_script(\"arguments[0].clcik();\", download)\n",
    "print('downloading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     get the link on the page\n",
    "# elems = driver.find_elements(By.XPATH, \"//a[contains(@class,'not-link')]\")\n",
    "# links = list(set([elem.get_attribute('href') for elem in elems]))\n",
    "# print(f'Scraper got {len(links)} links')\n",
    "\n",
    "# all_keys = driver.find_elements_by_xpath(\"//p[contains(@class,'mb-0 font-weight-300 font-size-0-9-rem')]\")\n",
    "# all_values = driver.find_elements(By.XPATH,\"//p[contains(@class, 'font-size-0-9-rem')]\")\n",
    "\n",
    "# values = get_attr(all_values)\n",
    "\n",
    "# with open('values.txt','w') as f:\n",
    "#     for _ in all_values:\n",
    "#         f.write(_.get_attribute(\"innerHTML\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNbPages(driver):\n",
    "    all_pages = driver.find_elements(By.XPATH,\"//a[@role='button'][contains(@class,'mr-3 pointer page-number ')]\")\n",
    "    return len(all_pages)\n",
    "\n",
    "def getLinksFromPage(driver):\n",
    "    elems = driver.find_elements(By.XPATH, \"//a[contains(@class,'not-link')]\")\n",
    "    links = list(set([elem.get_attribute('href') for elem in elems]))\n",
    "    return links\n",
    "\n",
    "def getPagesLinks(nom,prenom,driver,nb_pages=3, all_pages=False):\n",
    "\n",
    "    all_links = []\n",
    "    l1 = 'https://data.inpi.fr/search?advancedSearch=%257B%257D&filter=%257B%257D&nbResultsPerPage=250&order=asc'\n",
    "    l2 = \"&page=1&q=\"+prenom+'%20'+nom+'&sort=relevance&type=companies'\n",
    "    link = l1+l2\n",
    "    driver.get(link)\n",
    "    time.sleep(7)\n",
    "    nb_ = getNbPages(driver)\n",
    "\n",
    "    all_links.append(getLinksFromPage(driver))\n",
    "    \n",
    "    if all_pages or (nb_pages>= nb_):\n",
    "        for i in range(2,nb_+1):\n",
    "            driver.get(link.replace('page=1',f'page={i}'))\n",
    "            time.sleep(7)\n",
    "            all_links.append(getLinksFromPage(driver))\n",
    "    else:\n",
    "        for i in range(2,nb_pages+1):\n",
    "            driver.get(link.replace('page=1',f'page={i}'))\n",
    "            time.sleep(7)\n",
    "            all_links.append(getLinksFromPage(driver))\n",
    "    \n",
    "    all_links = [lk for page_likns in all_links for lk in page_likns]\n",
    "\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def get_attr(web_elements):\n",
    "    # return [_.get_attribute(\"innerHTML\") for _ in web_elements ]\n",
    "    return [el.text for el in web_elements] \n",
    "\n",
    "def list_dict_from_page(driver):\n",
    "    all_values = driver.find_elements(By.XPATH,\"//p[contains(@class, 'font-size-0-9-rem')]\")\n",
    "    values = get_attr(all_values)\n",
    "    \n",
    "    identity=[]\n",
    "    representants =[]\n",
    "    etablissements=[]\n",
    "    for i in range(len(values)):\n",
    "        if 'Pour plus d\\'informations sur les représentants' in values[i]:\n",
    "            identity = values[0:i]\n",
    "\n",
    "        if \"Type d'établissement\" in values[i]:\n",
    "            representants = values[len(identity)+1:i]   \n",
    "            etablissements=values[i:]\n",
    "            break\n",
    "\n",
    "    return identity,representants,etablissements\n",
    "\n",
    "def get_dict_identity(identity):\n",
    "    header = {}\n",
    "    for i in range(0,len(identity),2):\n",
    "        header.update({identity[i]:identity[i+1]})\n",
    "        \n",
    "    return header\n",
    "\n",
    "def getIndicesToDelete(keys):\n",
    "    return [i for i, j in enumerate(keys) if j == \"Nom d'usage\"]\n",
    "\n",
    "def dropNomDusage(ls, indices_to_delete):\n",
    "    clean_list = np.delete(ls, indices_to_delete).tolist()\n",
    "    return clean_list\n",
    "\n",
    "def getTheDict(keys,n_):\n",
    "    TheDict = {}\n",
    "\n",
    "    for j in range(0,len(keys),n_):\n",
    "        try:\n",
    "            i=next(ls)\n",
    "            rep[i] = {k:v for (k,v) in zip(keys[j:j+n_],values[j:j+n_])}\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "    return TheDict\n",
    "\n",
    "\n",
    "def get_dict_representative(representatives):\n",
    "\n",
    "    rep = {}\n",
    "    representatives = [re.sub('<\\w*>|\\n|</em>|<em class=\"highlight-elasticsearch\">', '',_).strip() for _ in representatives] \n",
    "    nb_rep_ = representatives.count('Nom, Prénom(s)')\n",
    "    \n",
    "    if nb_rep_==0:\n",
    "        return {\"Zero Message\":'Jiraya founds no representatives'}\n",
    "\n",
    "    else:\n",
    "        \n",
    "        ls = (k for k in range(1,nb_rep_+1))\n",
    "\n",
    "        keys = [representatives[i] for i in range(0,len(representatives),2)]\n",
    "        indices_to_delete = getIndicesToDelete(keys)\n",
    "        keys = dropNomDusage(keys,indices_to_delete)\n",
    "        values = dropNomDusage([representatives[i] for i in range(1,len(representatives),2)],indices_to_delete)\n",
    "\n",
    "        nb_rep_attr_ = int(len(keys)/nb_rep_)\n",
    "        \n",
    "        for j in range(0,len(keys),nb_rep_attr_):\n",
    "            try:\n",
    "                i=next(ls)\n",
    "                rep[i] = {k:v for (k,v) in zip(keys[j:j+nb_rep_attr_],values[j:j+nb_rep_attr_])}\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # rep = getTheDict(keys,nb_rep_attr_)\n",
    "        return  rep\n",
    "\n",
    "def get_dict_benef(driver):\n",
    "    all_keys = driver.find_elements(By.XPATH,\"//p[contains(@class, 'mb-0 font-size-13 inpi-light')]\")\n",
    "    all_values = driver.find_elements(By.XPATH,\"//p[contains(@class, 'font-size-15')]\")\n",
    "\n",
    "    keys = get_attr(all_keys)\n",
    "    values = get_attr(all_values)\n",
    "\n",
    "    nb_benef_ = keys.count('Nom prénom')\n",
    "    if nb_benef_==0:\n",
    "        return {'Zero Message':'Jiraya founds no beneficiaries'}\n",
    "    else : \n",
    "        nb_benf_attr_ = int(len(keys)/nb_benef_)\n",
    "\n",
    "        beneficiaires = {}\n",
    "        ls = (k for k in range(1,nb_benef_+1))\n",
    "    \n",
    "        for j in range(0,len(keys),nb_benf_attr_):\n",
    "            try:\n",
    "                i=next(ls)\n",
    "                beneficiaires[i] = {k:v for (k,v) in zip(keys[j:j+nb_benf_attr_],values[j:j+nb_benf_attr_])}\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        return beneficiaires\n",
    "\n",
    "def get_dict_ets(etablissements):\n",
    "\n",
    "    keys = [etablissements[i] for i in range(0,len(etablissements),2)]\n",
    "    values = [etablissements[i] for i in range(1,len(etablissements),2)]\n",
    "\n",
    "    nb_ets_ = keys.count(\"Type d'établissement\")\n",
    "    if nb_ets_ ==0:\n",
    "        return {'Zero message': 'Jiraya founds no etablissement'}\n",
    "    else:\n",
    "            \n",
    "        nb_ets_attr_ = int(len(keys)/nb_ets_)\n",
    "\n",
    "        etablissements = {}\n",
    "        ls = (k for k in range(1,nb_ets_+1))\n",
    "    \n",
    "        for j in range(0,len(keys),nb_ets_attr_):\n",
    "            try:\n",
    "                i=next(ls)\n",
    "                etablissements[i] = {k:v for (k,v) in zip(keys[j:j+nb_ets_attr_],values[j:j+nb_ets_attr_])}\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        return etablissements\n",
    "\n",
    "def dict_from_page(identity,representatives,driver,etablissements):\n",
    "    TheDict = { \"identity\": get_dict_identity(identity),\n",
    "                \"representatives\" : get_dict_representative(representatives),\n",
    "                \"beneficiaires\": get_dict_benef(driver),\n",
    "                \"etablissements\": get_dict_ets(etablissements)\n",
    "                                    }\n",
    "    return TheDict\n",
    "\n",
    "\n",
    "def run(nom,prenom,driver,all_pages=False):\n",
    "    links = getPagesLinks(nom,prenom,driver,nb_pages=1, all_pages=all_pages)\n",
    "    all_enterprise = dict()\n",
    "    for i in range(len(links)):\n",
    "        driver.get(links[i])\n",
    "        identity,representatives,etablissements = list_dict_from_page(driver)\n",
    "        all_enterprise[i] = dict_from_page(identity,representatives,driver,etablissements)\n",
    "\n",
    "    return all_enterprise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run('Mignon',\"Laurent\",driver,all_pages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = json.dump(test, open(\"test.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_scraper import inpi_data\n",
    "result = inpi_data(\"Mignon\",\"Laurent\").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk = getPagesLinks('DIDIER','SEBASTIEN',driver,nb_pages=3, all_pages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ca0a5c92fb2d3f8ca1f1ff1bbd4bb9014df2d10a104764019672e1a625de75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('scraping': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
